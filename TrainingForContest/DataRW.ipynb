{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a/b/c/d\n",
      "2:\n"
     ]
    }
   ],
   "source": [
    "txt = \"\\nGoogle#Runoob#Taobao#Facebook\"\n",
    "file=open('c:/DataSet_ML/input.txt','r+')\n",
    "file.write(txt)\n",
    "#print(file.mode)\n",
    "\n",
    "print(file.read(10))\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555\n",
      "777\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "string1='111,222,333\\n'\n",
    "string2='444:555:666\\n'\n",
    "\n",
    "a1=string1.strip().split(',')\n",
    "b1=string2.strip().split(':')\n",
    "\n",
    "for i in range(len(a1)):\n",
    "    print(int(a1[i])+int(b1[i]))\n",
    "\n",
    "#样例1\n",
    "# with  open(base_path+'input.txt') as f:\n",
    "#     line=f.readline()\n",
    "#     line\n",
    "# f=open(base_path)\n",
    "# for line in f:\n",
    "#     print(line)\n",
    "#样例2\n",
    "# lst = []\n",
    "# fp = open('data.txt')\n",
    "# for line in fp:\n",
    "# lineRev = line[::-1]\n",
    "# lst.append(line.strip()+'-'+lineRev.strip()+'\\n')\n",
    "# fp.close()\n",
    "# with open('data.txt', 'w') as fp:\n",
    "# fp.writelines(lst)\n",
    "\n",
    "# with open('companies.txt', 'r+') as f:\n",
    "# lines = f.readlines()\n",
    "# for i in range(len(lines)):\n",
    "# lines[i] = str(i+1) + ' ' + lines[i]\n",
    "# f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd']\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import linecache\n",
    "\n",
    "act=linecache.getline('c:/DataSet_ML/input.txt',1)\n",
    "a=act.strip().split('/')\n",
    "print(a)\n",
    "\n",
    "\n",
    "\n",
    "# actor=linecache.getline('c:/DataSet_ML/dataRW/data/film.txt',4)\n",
    "# actor=actor.strip('：').split('/')\n",
    "# print(actor)\n",
    "\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 16, 38, 5, 3, 3, 0, 2, 0, 0, 0, 0, 0, 0, 3, 3, 37, 18, 29, 13, 19, 20, 11, 12]\n",
      "1时间概率: 0.04526748971193416\n",
      "2时间概率: 0.06584362139917696\n",
      "3时间概率: 0.15637860082304528\n",
      "4时间概率: 0.0205761316872428\n",
      "5时间概率: 0.012345679012345678\n",
      "6时间概率: 0.012345679012345678\n",
      "7时间概率: 0.0\n",
      "8时间概率: 0.00823045267489712\n",
      "9时间概率: 0.0\n",
      "10时间概率: 0.0\n",
      "11时间概率: 0.0\n",
      "12时间概率: 0.0\n",
      "13时间概率: 0.0\n",
      "14时间概率: 0.0\n",
      "15时间概率: 0.012345679012345678\n",
      "16时间概率: 0.012345679012345678\n",
      "17时间概率: 0.1522633744855967\n",
      "18时间概率: 0.07407407407407407\n",
      "19时间概率: 0.11934156378600823\n",
      "20时间概率: 0.053497942386831275\n",
      "21时间概率: 0.07818930041152264\n",
      "22时间概率: 0.0823045267489712\n",
      "23时间概率: 0.04526748971193416\n",
      "24时间概率: 0.04938271604938271\n"
     ]
    }
   ],
   "source": [
    "class Gowalla:\n",
    "    def __init__(self,path):\n",
    "        self.path = path\n",
    "    \n",
    "    def input(self):\n",
    "        list_hour=[0 for item in range(24)]\n",
    "        #print(list_hour)\n",
    "        with open(self.path,'r') as f:\n",
    "            for line in f.readlines():\n",
    "                time=line.strip('\\n').split('\\t')[-1]\n",
    "                #print(time)\n",
    "                hour=int(time.split(':')[0])\n",
    "                #print(hour)\n",
    "                list_hour[hour]+=1\n",
    "        print(list_hour)\n",
    "        sum_hour=sum(list_hour)\n",
    "        for i in range(len(list_hour)):\n",
    "            pro=float(list_hour[i])/sum_hour\n",
    "            print (\"%d时间概率:\"%(i+1),pro)\n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    path='c:/DataSet_ML/dataRW/data/gowalla.txt'\n",
    "    gow=Gowalla(path)\n",
    "    gow.input()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在给定范围内的签到概率为： 0.17384999129879924\n",
      "time cost 0.02200007438659668 s\n",
      "2997 17239 0.17384999129879924\n",
      "time cost 0.0279998779296875 s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "class Trajectory:\n",
    "    def __init__(self,path):\n",
    "        self.path = path\n",
    "        \n",
    "    def input(self):\n",
    "        dlabel=pd.read_csv(self.path,usecols=['label'])\n",
    "        list_label=[value[0] for value in dlabel.values]\n",
    "        print(set(list_label))\n",
    "    \n",
    "    def col(self):\n",
    "        num_s=0\n",
    "        hour_n=0\n",
    "        dtime=pd.read_csv(self.path,usecols=['time'])\n",
    "        for value in dtime.values:\n",
    "            num_s+=1\n",
    "            hour=int(value[0].split(':')[0])\n",
    "            if 6<=hour<12:\n",
    "                  hour_n+=1\n",
    "        print(hour_n,num_s,float(hour_n)/num_s)\n",
    "    \n",
    "    def row(self):\n",
    "        #\n",
    "        time_start=time.time()\n",
    "        data_all=pd.read_csv(self.path)\n",
    "        #print(data_all['lat'])\n",
    "        data_select=data_all[(data_all['lat']<=40.01) & (data_all['lat']>=39.98) & (data_all['lng']<=116.41) & (data_all['lng']>=116.33)]\n",
    "        #print(data_all)\n",
    "        print('在给定范围内的签到概率为：',float(len(data_select))/len(data_all))\n",
    "        time_end=time.time()\n",
    "        print('time cost',time_end-time_start,'s')\n",
    "    \n",
    "    def row1(self):  \n",
    "        time_start=time.time()\n",
    "        record=0\n",
    "        select=0\n",
    "        with open (self.path,'r') as f:\n",
    "            for line in f.readlines()[1:]:\n",
    "                record+=1\n",
    "                line_list=line.strip('\\n').split(',')           \n",
    "                lat=float(line_list[0])\n",
    "                lng=float(line_list[1])\n",
    "                if 39.98 <= lat <= 40.01 and 116.33 <= lng <= 116.41:\n",
    "                    select+=1\n",
    "        \n",
    "        print(select,record,float(select)/record)\n",
    "        time_end=time.time()\n",
    "        print('time cost',time_end-time_start,'s')\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    path='c:/DataSet_ML/dataRW/data/trajectory.csv'\n",
    "    tra=Trajectory(path)\n",
    "    tra.row()\n",
    "    tra.row1()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.0\n",
      "85.0\n",
      "81.0\n",
      "74.0\n",
      "75.0\n",
      "56.0\n",
      "103.0\n",
      "67.0\n",
      "77.0\n",
      "57.0\n",
      "72.0\n",
      "sss 103.0\n",
      "项目名： Select 36.0\n",
      "项目名： Input 14.0\n",
      "项目名： Output 26.0\n",
      "项目名： Time 12.0\n",
      "项目名： Location 10.0\n",
      "项目名： Filter 14.0\n",
      "7\n",
      "['dom', 25.0, 13.0, 18.0, 10.0, 8.0, 12.0]\n",
      "['Input', 13.0, 11.0, 8.0, 11.0, 11.0, 10.0, 14.0, 4.0, 8.0, 9.0, 7.0]\n",
      "13.0\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import xlrd\n",
    "\n",
    "class Excel:\n",
    "\n",
    "    def __init__(self, data_path, sheetname):\n",
    "        self.data_path = data_path\n",
    "        self.sheetname = sheetname\n",
    "    \n",
    "    def readExcel(self):\n",
    "        data = xlrd.open_workbook(self.data_path)   \n",
    "        table = data.sheet_by_name(self.sheetname)  \n",
    "        print(table.ncols)\n",
    "        print(table.row_values(1))\n",
    "        print(table.col_values(2))\n",
    "        print(table.cell(1,2).value)\n",
    "\n",
    "\n",
    "    def max_ss(self):\n",
    "        data = xlrd.open_workbook(self.data_path)   # open the excel\n",
    "        table = data.sheet_by_name(self.sheetname)  # load data\n",
    "        use=''\n",
    "        max_score=0\n",
    "        for i in range(1,table.nrows):\n",
    "            sum_score=0\n",
    "            for j in range(1,table.ncols):\n",
    "                sum_score+=table.cell(i,j).value\n",
    "            print(sum_score)\n",
    "            if sum_score>max_score:\n",
    "                max_score=sum_score\n",
    "                use=table.cell(i,0).value\n",
    "        print(use,max_score)\n",
    "\n",
    "    def item(self):                           \n",
    "        data = xlrd.open_workbook(self.data_path)   # open the excel\n",
    "        table = data.sheet_by_name(self.sheetname)  # load data\n",
    "\n",
    "        for i in range(1, table.ncols):\n",
    "            print (\"项目名：\", table.col_values(i)[0], max(table.col_values(i)[1:]))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    data_path = 'c:/DataSet_ML/dataRW/data/score.xlsx.'\n",
    "    sheetname = \"Sheet1\"\n",
    "    excel = Excel(data_path, sheetname)\n",
    "    excel.max_ss()\n",
    "    excel.item()\n",
    "    excel.readExcel()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "path_out = \"c:/DataSet_ML/result1.txt\"\n",
    "outfile = open(path_out, \"w\")\n",
    "\n",
    "for index in range(100):\n",
    "    lat = random.uniform(39.11, 42.21)\n",
    "    lng = random.uniform(116.25, 118.33)\n",
    "    hour=random.randint(0,23)\n",
    "    minu=random.randint(0,59)\n",
    "    sec=random.randint(0,59)\n",
    "    outfile.write(str(lat)+'\\t'+str(lng)+'\\t'+str(hour)+':'+str(minu)+':'+str(sec)+'\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import csv\n",
    "import random\n",
    "\n",
    "path_out='c:/DataSet_ML/dataRW/data/result2'\n",
    "with open(path_out,'w',newline='') as file:#newline解决自动空一行\n",
    "    write = csv.writer(file)\n",
    "    for index in range(100):\n",
    "        lat = random.uniform(39.11, 42.21)\n",
    "        lng = random.uniform(116.25, 118.33)\n",
    "        hour=random.randint(0,23)\n",
    "        minu=random.randint(0,59)\n",
    "        sec=random.randint(0,59)\n",
    "        list_line=[str(lat),str(lng),str(hour),str(minu),str(sec)]\n",
    "        #print(list_line)\n",
    "        write.writerow(list_line)\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n",
    "path_out='c:/DataSet_ML/dataRW/data/result3'\n",
    "with open(path_out,'w') as file:\n",
    "    for index in range(100):\n",
    "        lat = random.uniform(39.11, 42.21)\n",
    "        lng = random.uniform(116.25, 118.33)\n",
    "        hour=random.randint(0,23)\n",
    "        minu=random.randint(0,59)\n",
    "        sec=random.randint(0,59)\n",
    "        file.write(str(lat)+','+str(lng)+','+str(hour)+':'+str(minu)+':'+str(sec)+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import xlsxwriter\n",
    "\n",
    "workbook = xlsxwriter.Workbook('D:\\\\result.xlsx')  # 创建一个Excel文件\n",
    "worksheet = workbook.add_worksheet()               # 创建一个sheet\n",
    "\n",
    "title = ['Id', 'Score']     # 表格title\n",
    "worksheet.write_row('A1', title)                    # title 写入Excel\n",
    "\n",
    "for i in range(1, 10):\n",
    "    num0 = bytes(i+1)\n",
    "    num = bytes(i)\n",
    "    row = 'A' + num0\n",
    "    data = ['name' + num, num]\n",
    "    worksheet.write_row(row, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '34.0172734606', '118.447508812', '17:18:59']\n",
      "['0', '34.0426141541', '118.26962471', '00:29:04']\n",
      "['0', '34.0577193758', '118.412960574', '16:34:09']\n",
      "['0', '34.0631390112', '118.297476769', '21:18:21']\n",
      "['0', '34.061429', '118.30636315', '21:14:58']\n",
      "['0', '34.0172734606', '118.447508812', '17:02:14']\n",
      "['0', '34.0506333752', '118.258742668', '23:57:54']\n",
      "['0', '34.0400626446', '118.270407915', '16:22:29']\n",
      "['0', '34.030414819', '118.270817569', '20:38:38']\n",
      "['0', '34.0171002376', '118.282734258', '20:34:27']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "workbook = xlsxwriter.Workbook('c:/DataSet_ML/dataRW/data/result4.xlsx') \n",
    "worksheet = workbook.add_worksheet()               \n",
    "\n",
    "\n",
    "infile = open('c:/DataSet_ML/dataRW/data/gowalla.txt',\"r\").readlines()[0:10]\n",
    "title = ['id','lat','lng','time'] \n",
    "worksheet.write_row('A1', title)  \n",
    "\n",
    "\n",
    "for i in range(len(infile)):\n",
    "    list_line = infile[i].strip(\"\\n\").split(\"\\t\")\n",
    "    print(list_line)\n",
    "    row = 'A' + str(i+2)\n",
    "    worksheet.write_row(row, list_line)\n",
    "\n",
    "\n",
    "data = ('Foo', 'Bar', 'Baz')\n",
    "worksheet.write_column('E1',data)\n",
    "worksheet.write('E3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1    2\n",
      "a    2\n",
      "2    1\n",
      "3    1\n",
      "c    1\n",
      "b    1\n",
      "d    1\n",
      "dtype: int64\n",
      "Index(['1', 'a', '2', '3', 'c', 'b', 'd'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "a = [1, 2, 3, 1, 1, 2]\n",
    "result = Counter(list1)\n",
    "\n",
    "print(result['a'])\n",
    "list1=['b','c','a','d','a',1,2,3,1]\n",
    "\n",
    "import pandas as pd\n",
    "a = [1, 2, 3, 1, 1, 2]\n",
    "result = pd.value_counts(list1)\n",
    "print(result)\n",
    "\n",
    "print(result.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word', 'asd', 'jifn', 'dw', 'asd']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getTxt(): #3对文本预处理（包括）\n",
    "    txt = open('C:/DataSet_ML/head.txt').read()  #2.通过文件读取字符串 str\n",
    "    txt = txt.lower()#将所有的单词全部转化成小写\n",
    "    for ch in \",.!、!@#$%^'\": #将所有除了单词以外的符号换成空格\n",
    "        txt.replace(ch, ' ')\n",
    "    return txt\n",
    "\n",
    "txtArr = getTxt().strip('\\n').split()\n",
    "print(txtArr)\n",
    "\n",
    "\n",
    "# counts = {}\n",
    "# for word in txtArr:\n",
    "#     counts[word] = counts.get(word, 0) + 1\n",
    "# countsList = list(counts.items())\n",
    "# countsList.sort(key=lambda x:x[1], reverse=True)\n",
    "# for i in range(3):\n",
    "#     word, count = countsList[i]\n",
    "#     print(word,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17624\n",
      "168 48 17624\n",
      "用户在[39.96,116.30—39.98,116.32]区间出现的概率为: 0.009532455742169769\n",
      "用户在[39.99,116.33—40.11,116.35]区间出现的概率为: 0.0027235587834770767\n",
      "用户在以下时间段出现的概率[11:00:00—13:00:00): 0.057421697684975036\n",
      "用户在以下时间段出现的概率[17:00:00—21:00:00): 0.4634021788470268\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Trajectory:\n",
    "    def __init__(self,path):\n",
    "        self.path = path\n",
    "        \n",
    "    def fun1(self):\n",
    "        len=0\n",
    "        with open(self.path) as f:\n",
    "            for line in f.readlines():\n",
    "                len+=1\n",
    "        print(len)\n",
    "   \n",
    "    def fun2(self):        \n",
    "        record=0\n",
    "        select1=0\n",
    "        select2=0\n",
    "        line_list=[]\n",
    "        lat=[]\n",
    "        lng=[]\n",
    "        with open (self.path,'r') as f:\n",
    "            for line in f.readlines():\n",
    "                record+=1\n",
    "                line_list.append(line.strip('\\n').split('\\t'))                \n",
    "            for i in range(len(line_list)):\n",
    "                lat.append(float(line_list[i][1]))\n",
    "                lng.append(float(line_list[i][2]))\n",
    "            for j in range(len(lat)):\n",
    "                if 39.96 <= lat[j] <= 39.98 and 116.30 <= lng[j] <= 116.32:\n",
    "                    select1+=1\n",
    "                if 39.99 <= lat[j] <= 40.11 and 116.33 <= lng[j] <= 116.35:\n",
    "                    select2+=1\n",
    "        print(select1,select2,record)\n",
    "        print('用户在[39.96,116.30—39.98,116.32]区间出现的概率为:',float(select1)/record)\n",
    "        print('用户在[39.99,116.33—40.11,116.35]区间出现的概率为:',float(select2)/record)\n",
    "    \n",
    "    def fun3(self):\n",
    "        num_s=0\n",
    "        hour=[]\n",
    "        time=[]\n",
    "        line_list=[]\n",
    "        select1=0\n",
    "        select2=0\n",
    "        with open (self.path,'r') as f:\n",
    "            for line in f.readlines():\n",
    "                num_s+=1\n",
    "                line_list.append(line.strip('\\n').split('\\t')) \n",
    "            for i in range(len(line_list)):\n",
    "                time.append(line_list[i][3])  \n",
    "                hour.append(int(time[i].split(':')[0]))\n",
    "            \n",
    "            for j in range(len(hour)):\n",
    "                if 11<= hour[j]<13:\n",
    "                    select1+=1\n",
    "                if 17<= hour[j]<21:\n",
    "                    select2+=1\n",
    "        print('用户在以下时间段出现的概率[11:00:00—13:00:00):',float(select1)/num_s)\n",
    "        print('用户在以下时间段出现的概率[17:00:00—21:00:00):',float(select2)/num_s)\n",
    "          \n",
    "if __name__ == \"__main__\":\n",
    "    path='c:/DataSet_ML/dataRW/data/Test/trajectory.txt'\n",
    "    tra=Trajectory(path)\n",
    "    tra.fun1()\n",
    "    tra.fun2()\n",
    "    tra.fun3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "# import xlsxwriter\n",
    "# class Checkin:\n",
    "#     def __init__(self,path):\n",
    "#         self.path = path\n",
    "        \n",
    "#     def read(self):\n",
    "#         data_all=pd.read_csv(self.path,header=None)\n",
    "#         word_list=[]\n",
    "#         data_id=data_all[0]\n",
    "#         data_id=set(data_id)\n",
    "#         data_word=data_all[1]      \n",
    "#         data=[]\n",
    "        \n",
    "#         for i in range(len(data_id)):\n",
    "#             data.append(data_all[data_all[0]==i])\n",
    "        \n",
    "#         for k in range(len(data)): \n",
    "#             word_x=[]\n",
    "#             for value in data[k][1].values:                \n",
    "#                 for word in value.strip('').split('|'):\n",
    "#                     word_x.append(word)\n",
    "#                     word_x=list(filter(None, word_x))\n",
    "#             word_list.append(word_x)\n",
    "              \n",
    "#         result=[]\n",
    "#         for i in range(len(word_list)):          \n",
    "#             result.append(pd.value_counts(word_list[i]))\n",
    "        \n",
    "#         #print((result[0].index)[0])\n",
    "#         pro=[]\n",
    "#         max_w=[]\n",
    "#         use=list(data_id)\n",
    "#         for i in range(len(result)):\n",
    "#             pro.append(float(result[i][0])/sum(result[i]))\n",
    "#             max_w.append((result[i].index)[0])\n",
    "#         #print(max_w[0],pro[0])\n",
    "#         final=[]\n",
    "#         for i in range(len(pro)):\n",
    "#             lst=[use[i],max_w[i],pro[i]]\n",
    "#             final.append(lst)\n",
    "#         print(result[5])\n",
    "        \n",
    "        \n",
    "#         workbook = xlsxwriter.Workbook('c:/DataSet_ML/dataRW/data/Test/result1.xlsx') \n",
    "#         worksheet = workbook.add_worksheet() \n",
    "#         for i in range(len(final)):\n",
    "#             row='A'+str(i+1)\n",
    "#             worksheet.write_row(row,final[i])\n",
    "#         workbook.close()\n",
    "\n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "#     path='c:/DataSet_ML/dataRW/data/Test/checkin.csv'\n",
    "#     cc=Checkin(path)\n",
    "#     cc.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题目：数据文件checkin.csv是经过处理的社交网络文本数据，每行包括两个字段，第一个为用户id,第二个是根据用户推文提取的单词，单词之间用|分隔。要求如下：\n",
    "1.定义一个类Checkin\n",
    "2.定义成员函数read()，完成数据的读取，计算用户在不同单词上的概率，找到最大概率的单词及对应的概率\n",
    "3.将上一步的结果写入到本地文件result.xlsx，部分结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coffee', 'espresso', 'rest', 'stop', 'shop', 'shops', 'atm', 'convenience', 'store', 'gas', 'deli', 'bodega', 'delis', 'bodegas', 'casino', 'concert', 'hall', 'entertainment', 'gambling', 'gym', 'hotel', 'karaoke', 'music', 'venue', 'nightclub', 'nightlife', 'pizza', 'poker', 'pool', 'shopping', 'slots', 'spa', 'vh1', 'casinos', 'resort', 'resorts', 'hotels', 'borgata', 'noodles', 'pho', 'ramen', 'noodle', 'house', 'chinese', 'restaurant', 'restaurants', 'asian', 'fine', 'employees', 'food', 'grocery', 'organic', 'shopping', 'total', 'babes', 'whole', 'foods', 'yummy', 'store', 'stores', 'deli', 'bodega', 'delis', 'bodegas', 'boutiques', 'fashionista', 'photobooth', 'shop', 'mall', 'malls', 'american', 'restaurant', 'restaurants', 'coffee', 'shop', 'shops', 'caf', 'cafs', 'neighborhood', 'market', 'supercenter', 'walmart', 'wally', 'world', 'department', 'store', 'stores', 'miscellaneous', 'shop', 'shops', 'barbeque', 'american', 'restaurant', 'restaurants', 'apple', 'cameras', 'electronics', 'geek', 'squad', 'hardware', 'music', 'phones', 'retail', 'shopping', 'software', 'stereo', 'tv', 'video', 'games', 'wifi', 'xbox', 'store', 'stores', 'grocery', 'store', 'stores', 'mexican', 'restaurant', 'restaurants', 'coffee', 'doughnut', 'donut', 'shop', 'shops', 'donuts', 'neighborhood', 'market', 'supercenter', 'walmart', 'wally', 'world', 'food', 'shopping', 'department', 'store', 'stores', 'miscellaneous', 'shop', 'shops', 'parking', 'college', 'football', 'indiana', 'university', 'iu', 'sports', 'stadium', 'stadiums', 'field', 'fields', 'bar', 'beer', 'pong', 'college', 'kok', 'patios', 'tvs', 'bars', 'sports', 'cocktail', 'american', 'restaurant', 'restaurants', 'beer', 'grilled', 'cheese', 'trendy', 'vegan', 'friendly', 'vegetarian', 'sandwich', 'place', 'places', 'sandwiches', 'bar', 'bars', 'bodies', 'cleveland', 'exhibit', 'exhibition', 'east', '4th', 'education', 'gallery', 'general', 'entertainment', 'college', 'university', 'colleges', 'universities', 'gallery', 'hall', 'of', 'fame', 'historychannel', 'museum', 'music', 'venue', 'rock', 'and', 'roll', 'the', 'examiner', 'museums', 'venues', 'atm', 'beer', 'bravo', 'cash', 'only', 'food', 'fries', 'landmark', 'sandwich', 'place', 'places', 'sandwiches', 'bar', 'bars', 'american', 'restaurant', 'restaurants', 'casino', 'concert', 'hall', 'entertainment', 'gambling', 'gym', 'hotel', 'karaoke', 'music', 'venue', 'nightclub', 'nightlife', 'pizza', 'poker', 'pool', 'shopping', 'slots', 'spa', 'vh1', 'casinos', 'resort', 'resorts', 'hotels', 'borgata', 'noodles', 'pho', 'ramen', 'noodle', 'house', 'chinese', 'restaurant', 'restaurants', 'asian', 'autocross', 'cheerleaders', 'concert', 'cones', 'fedexfield', 'football', 'hot', 'dogs', 'ichsan', 'rachman', 'live', 'men', 'nfl', 'quad', 'redskins', 'scca', 'soccer', 'solo', 'stadium', 'stadiums']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "class Chenkin:    \n",
    "    _final=[]#存储[用户，最大单词，概率]\n",
    "    def __init__(self,_path_in):\n",
    "        self._path_in=_path_in\n",
    "    \n",
    "    def read(self):      \n",
    "        data_in=pd.read_csv(self._path_in,header=None)\n",
    "        \n",
    "        len_all=len(data_in)#数据总长度\n",
    "        len_use=len(set(data_in.iloc[:,0]))#用户数\n",
    "        \n",
    "        user_id=list(set(data_in.iloc[:,0]))#用户列表\n",
    "        pro=[]                              #最大单词概率\n",
    "        max_word=[]                         #最大单词列表\n",
    "        \n",
    "        data_cut=[]                         #按用户切分数据集\n",
    "        for i in range(len_all):\n",
    "            data_cut.append(data_in[data_in[0]==i])\n",
    "        \n",
    "        word_list=[]#每个用户的单词列表\n",
    "\n",
    "       \n",
    "        for i in range(len_use):\n",
    "            word_temp=[]#临时存放每一行word\n",
    "            for line in data_cut[i][1].values:\n",
    "                for word in line.split('|'):\n",
    "                    word_temp.append(word)\n",
    "                    word_temp=list(filter(None, word_temp))#删除空值\n",
    "            word_list.append(word_temp)\n",
    "        \n",
    "        word_list_fre=[]##统计次词频\n",
    "        for i in range(len_use):          \n",
    "            result.append(pd.value_counts(word_list[i]))\n",
    "        \n",
    "        #print((result[0].index)[0])\n",
    "\n",
    "if __name__=='__main__':\n",
    "    path_in='c:/DataSet_ML/dataRW/data/Test/checkin.csv'\n",
    "    ck=Chenkin(path_in)\n",
    "    ck.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from collections import Counter\n",
    "# from openpyxl import Workbook\n",
    "\n",
    "# class Checkin:\n",
    "#     def __init__(self,path='c:/DataSet_ML/dataRW/data/Test/checkin.csv'):\n",
    "#         self._path = path\n",
    "           \n",
    "#     def read(self):\n",
    "#         self.users = set()\n",
    "#         self.user_words = dict()\n",
    "#         self.user_word_freq = []\n",
    "#         with open(path,'r') as f:\n",
    "#             for item in f.readlines():\n",
    "#                 user = item.split(',')[0]\n",
    "#                 self.users.add(user)\n",
    "#                 lst = self.user_words.get(user,[])\n",
    "#                 wlst = item.split(',')[1].strip().split('|')\n",
    "#                 wlst = list(filter(None, wlst))\n",
    "#                 lst.extend(wlst)\n",
    "#                 self.user_words[user] = lst      \n",
    "#         self.__word_freq()\n",
    "#         self.__write()\n",
    "       \n",
    "#     #计算单词频率\n",
    "#     def __word_freq(self):\n",
    "#         for u in self.users:\n",
    "#             words = self.user_words[u]\n",
    "#             max_freq_word = max(set(words), key = words.count)\n",
    "#             freq = words.count(max_freq_word)/len(words)\n",
    "#             self.user_word_freq.append((u,max_freq_word,freq))\n",
    "#         self.user_word_freq.sort(key = lambda x:int(x[0]))\n",
    "#     #写文件  \n",
    "#     def __write(self):\n",
    "#         workbook = Workbook()\n",
    "#         booksheet = workbook.active  \n",
    "#         for i in range(len(self.user_word_freq)):\n",
    "#             utuple = self.user_word_freq[i]\n",
    "#             booksheet.append(utuple)\n",
    "#         workbook.save('c:/DataSet_ML/dataRW/data/Test/result.xlsx')\n",
    "        \n",
    "           \n",
    "# if __name__ == \"__main__\":\n",
    "#     path='c:/DataSet_ML/dataRW/data/Test/checkin.csv'\n",
    "#     cc=Checkin()\n",
    "#     cc.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
