{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <center>自然语言处理(NLP)</center>\n",
    " ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    1.概述\n",
    "##    2.分词\n",
    "##    3.词向量\n",
    "##    4.RNN/LSTM\n",
    "##    5.示例\n",
    "##    6.小结\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.概述\n",
    "* AI技术分类\n",
    "  * 机器视觉\n",
    "  * 自然语言处理\n",
    "  * 知识图谱\n",
    "  \n",
    "  \n",
    "\n",
    "* NLP的主要应用\n",
    "  * 机器翻译 \n",
    "      * 文本翻译  Google、百度、有道等公司都有多语言翻译系统\n",
    "      * 语音翻译  搜狗推出的机器同传在会议场景出现，演讲者的语音实时转换成文本，并且进行同步翻译\n",
    "      * 图像翻译  谷歌、微软、Facebook 和百度均拥有能够让用户搜索或者自动整理没有识别标签照片的技术\n",
    "  * 信息检索  \n",
    "      * 谷歌为代表的「关键词查询+选择性浏览」搜索引擎交互方式\n",
    "  * 自动问答\n",
    "      * 电商平台智能客服\n",
    "      * 聊天机器人\n",
    "  * 文本摘要\n",
    "      * 新闻标题自动生成\n",
    "  * 情感信息\n",
    "      * 分析评价是好评还是差评   \n",
    "      \n",
    "      \n",
    "\n",
    "* NLP基本处理流程\n",
    "  1. 获取语料  \n",
    "    语料是语言学研究的内容。语料是构成语料库的基本单元。\n",
    "  2. 语料预处理\n",
    "    在一个完整的中文自然语言处理工程应用中，语料预处理大概会占到整个50%-70%的工作量。下面通过数据洗清、分词、词性标注、去停用词四个大的方面来完成语料的预处理工作。  \n",
    "    1. 语料清洗  \n",
    "    在语料中找到我们感兴趣的东西，包括对于原始文本提取标题、摘要、正文等信息\n",
    "    2. <big>***分词***</big>  \n",
    "    进行文本挖掘分析时，我们希望文本处理的最小单位粒度是词或者词语，所以这个时候就需要分词来将文本全部进行分词。\n",
    "    3. 词性标注  \n",
    "    词性标注，就是给每个词或者词语打词类标签，如形容词、动词、名词等。词性标注不是非必需的，比如常见的文本分类就不用关心词性问题。\n",
    "    4. 去停用词  \n",
    "    停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。  \n",
    "  3. <big>***词向量***</big>  \n",
    "    做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。显然，如果要计算我们至少需要把中文分词的字符串转换成数字，确切的说应该是数学中的向量。有两种常用的表示模型分别是词袋模型和词向量。\n",
    "  4. <big>***模型训练***</big>  \n",
    "    在特征向量选择好之后，接下来要做的事情当然就是训练模型，对于不同的应用需求，我们使用不同的模型,常用的深度学习模型有RNN、LSTM等。\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.分词\n",
    "* 示例：  \n",
    "  *&nbsp;&nbsp;&nbsp;&nbsp;我来到江苏宜兴*  \n",
    "  *&nbsp;&nbsp;&nbsp;&nbsp;I came to Yixing, Jiangsu Province*  \n",
    "  中文分词需要将一个汉字序列切分成一个个单独的词。 \n",
    "  \n",
    "  \n",
    "* 常用中文分词工具  \n",
    "  *  **jieba分词**  \n",
    "  jieba分词是国内使用人数最多的中文分词工具（github链接：https://github.com/fxsjy/jieba )。  \n",
    "  jieba分词支持三种模式：  \n",
    "    * 精确模式：试图将句子最精确地切开，适合文本分析；\n",
    "    * 全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；\n",
    "    * 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\n",
    "\n",
    "  *  **thulac**  \n",
    "  THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包（github链接：https://github.com/thunlp/THULAC-Python ），具有中文分词和词性标注功能。  \n",
    "  THULAC具有如下几个特点：  \n",
    "    * 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。\n",
    "    * 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。\n",
    "    * 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。\n",
    "  *  **SnowNLP**  \n",
    "  SnowNLP是一个python写的类库(https://github.com/isnowfy/snownlp )，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的。SnowNLP主要包括如下几个功能：  \n",
    "    * 中文分词（Character-Based Generative Model）；\n",
    "    * 词性标注（3-gram HMM）；\n",
    "    * 情感分析（简单分析，如评价信息）；\n",
    "    * 文本分类（Naive Bayes）\n",
    "    * 转换成拼音（Trie树实现的最大匹配）\n",
    "    * 繁简转换（Trie树实现的最大匹配）\n",
    "    * 文本关键词和文本摘要提取（TextRank算法）\n",
    "    * 计算文档词频（TF，Term Frequency）和逆向文档频率（IDF，Inverse Document Frequency）\n",
    "    * Tokenization（分割成句子）\n",
    "    * 文本相似度计算（BM25）\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba分词\n",
    "1. **使用方法：**  \n",
    " ``` python\n",
    "    import jieba\n",
    "    jieba.cut(str) #str为中文字符串  \n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.864 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 来到 江苏 宜兴\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我来到江苏宜兴\")\n",
    "print(\" \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  2. **添加自定义词典**  \n",
    "\n",
    "    **载入词典**  \n",
    "\n",
    "    * 开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率  \n",
    "    * 用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径  \n",
    "    * 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。  \n",
    "    * 词频省略时使用自动计算的能保证分出该词的词频。  \n",
    "例如\n",
    "   > 创新办 3 i  \n",
    "    云计算 5  \n",
    "    \n",
    "    示例：李小福是创新办主任，也是云计算方面的专家"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李小福 是 创新 办 主任 ， 也 是 云 计算 方面 的 专家\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "s = \"李小福是创新办主任，也是云计算方面的专家\"\n",
    "seg_list = jieba.cut(s)\n",
    "print(\" \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!cat dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dict.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c407dcdede87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"李小福是创新办主任，也是云计算方面的专家\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_userdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dict.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mseg_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mload_userdict\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[0mf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolve_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dict.txt'"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "s = \"李小福是创新办主任，也是云计算方面的专家\"\n",
    "jieba.load_userdict('dict.txt')\n",
    "seg_list = jieba.cut(s)\n",
    "print(\" \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  3. **停用词处理**   \n",
    "  停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['!', '\"', '#', '$', '%', '&', '0', '1', '2', '3', '4', '5', '一', '一.', '一一',\n",
    "             '一下', '一个', '一些', '一何', '一切', '一则', '一则通过', '一天', '一定']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat stopwords.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  4. **词云**  \n",
    "  ![词云图](wordcloudexample.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词云\n",
    "1. **使用方法：**  \n",
    " ``` python\n",
    "    from wordcloud import WordCloud\n",
    "    wordcloud = WordCloud(font_path=\"simsun.ttf\").generate(text)   #text为分词处理后的文本 \n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "mytext = '建设 社会 提高 文化 服务 产业 坚持 不断 保护 养老 建设 提高 建设 建设 提高 建设 进一步'\n",
    "wordcloud = WordCloud(font_path=\"simsun.ttf\").generate(mytext)\n",
    "#wordcloud = WordCloud().generate(mytext)\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\") \n",
    "mytext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  练习：  \n",
    "* 任意选择一篇中文文章，对其进行：  \n",
    "  1、分词  \n",
    "  2、去停用词  \n",
    "  3、绘制词云图  \n",
    "  4、通过网络，学习绘制不同形状的词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba # \n",
    "#jieba.load_userdict('userdict.txt')  # 创建停用词list  \n",
    "def stopwordslist(filepath): \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()] \n",
    "    return stopwords # 对句子进行分词  \n",
    "\n",
    "def seg_sentence(sentence): \n",
    "    sentence_seged = jieba.cut(sentence.strip()) \n",
    "    stopwords = stopwordslist('./stopwords.txt') # 这里加载停用词的路径  \n",
    "    outstr = '' \n",
    "    for word in sentence_seged: \n",
    "        if word not in stopwords: \n",
    "            if word != '\\t': \n",
    "                outstr += word \n",
    "                outstr += \" \" \n",
    "    return outstr \n",
    "\n",
    "inputs = open('./input.txt', 'r', encoding='utf-8') \n",
    "outputs = open('./output.txt', 'w') \n",
    "for line in inputs: \n",
    "    line_seg = seg_sentence(line) # 这里的返回值是字符串  \n",
    "    outputs.write(line_seg + '\\n') \n",
    "outputs.close() \n",
    "inputs.close() \n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "myfile = open('./output.txt','r')\n",
    "mytext = ''\n",
    "for line in myfile:\n",
    "    mytext+=line\n",
    "wordcloud = WordCloud(font_path=\"simsun.ttf\").generate(mytext)\n",
    "#wordcloud = WordCloud().generate(mytext)\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\") \n",
    "mytext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.词向量\n",
    "计算机无法直接识别中文的字和词语，计算机只能识别数字0、1，为了使计算机能够对中文进行识别，我们需要对中文词语进行编码\n",
    "word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。虽然源码是开源的，但是谷歌的代码库国内无法访问，因此本文的讲解word2vec原理以Github上的word2vec代码为准。本文关注于word2vec的基础知识。\n",
    "\n",
    "* 1.one-hot编码  \n",
    "  **示例:**\n",
    "  > 性别特征： [\"男\",\"女\"]（N=2）  \n",
    "    男  =>  10  \n",
    "    女  =>  01  \n",
    "\n",
    "  > 国家特征：[\"中国\"，\"美国，\"法国\"]（N=3）  \n",
    "    中国  =>  100  \n",
    "    美国  =>  010  \n",
    "    法国  =>  001  \n",
    "\n",
    "  > 运动特征：[\"足球\"，\"篮球\"，\"羽毛球\"，\"乒乓球\"]（N=4）  \n",
    "    足球  =>  1000  \n",
    "    篮球  =>  0100  \n",
    "    羽毛球  =>  0010  \n",
    "    乒乓球  =>  0001  \n",
    "    \n",
    "  * 编码的维度跟词汇表大小一致  \n",
    "  * 稀疏矩阵  \n",
    "  * 缺点  \n",
    "    * 无法使用该方法进行单词之间的相似度计算。\n",
    "    * 词向量可能非常长,计算量太大    \n",
    "    * 词汇表增加新的单词的时候，所有的词需要重新编码\n",
    "    \n",
    "* 2.连续词袋模型（continuous bag of words，CBOW）\n",
    "  通过上下文来推断中心词。\n",
    "\n",
    "* 3.跳字模型（skip-gram）\n",
    "  通过中心词来推断上下文一定窗口内的单词。\n",
    "   \n",
    "  假设文本：“我昨天上学迟到了，老师批评了我”  \n",
    "  做分词、去停用词处理后：“我 昨天 上学 迟到 老师 批评 我”  \n",
    "  假设我们的窗口大小skip-window=2，中心词为“迟到”，那么上下文的词即为：“昨天”、“上学”、“老师”、”批评“。这里的上下文词又被称作“背景词”，对应的窗口称作“背景窗口”。\n",
    "  \n",
    "  跳字模型能帮我们做的就是，通过中心词“迟到”，生成与它距离不超过2的背景词“昨天”、“上学”、“老师”、”批评“的条件概率，用公式表示即：\n",
    "  <h3><center>P(“昨天&quot;,“上学&quot;,“老师&quot;,“批评&quot;∣“迟到&quot;)</center></h3>\n",
    "  假设给定中心词的情况下，背景词之间是相互独立的，公式可以进一步得到：\n",
    "  <h3><center>P(“昨天&quot;∣“迟到&quot;)⋅P(“上学&quot;∣“迟到&quot;)⋅P(“老师&quot;∣“迟到&quot;)⋅P(“批评&quot;∣“迟到&quot;)  </center></h3>\n",
    "  这里是一个一对多的情景，根据一个词来推测2m个词，（m表示背景窗口的大小）。\n",
    "  \n",
    "  ![词向量训练过程图](word2vec1.png)  \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "      1. one-hot编码。\n",
    "        比如我这里的文本序列为：\"我 昨天 上学 迟到 老师 批评 我\"，那么可以进行如下编码了。\n",
    "        >我 ：[1,0,0,0,0,0]  \n",
    "        昨天：[0,1,0,0,0,0]  \n",
    "        上学：[0,0,1,0,0,0]  \n",
    "        迟到：[0,0,0,1,0,0]  \n",
    "        老师：[0,0,0,0,1,0]   \n",
    "        批评：[0,0,0,0,0,1] \n",
    "      2. lookup table查表降维。根据索引映射，将每个单词映射到d维空间，通过这样的方式就可以将所有的单词映射到矩阵W上（矩阵的形状为V∗d），并且每个单词与矩阵中的某列一一对应。   \n",
    "      \n",
    "    \n",
    " ![查表图](word2vec3.png) \n",
    "    \n",
    "      3. skip-gram模型训练。\n",
    "      运用哈夫曼树等算法进行模型训练\n",
    "      \n",
    "* 4.fasttext（facebook预训练词向量）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "zh_model = gensim.models.KeyedVectors.load_word2vec_format('cc.zh.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for word in zh_model.vocab:\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[1000:1030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(zh_model[words[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[100], zh_model[words[100]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[10], zh_model[words[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_model.similar_by_word('学生', topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_model.similar_by_word('电网', topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**示例：**  \n",
    "  国王 - 男人 = 王后 - ?  =>  ？= 男人 + 王后 - 国王    \n",
    "  日本 - 东京 = ? - 北京  =>  ？= 日本 + 北京 - 东京 \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_add = ['男人', '王后']\n",
    "word_sub = ['国王']\n",
    "for word in zh_model.most_similar(positive = word_add, negative = word_sub, topn = 2):\n",
    "    print(word[0], word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_add = ['日本', '北京']\n",
    "word_sub = ['东京']\n",
    "for word in zh_model.most_similar(positive = word_add, negative = word_sub, topn = 5):\n",
    "    print(word[0], word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 我们把词向量降维压缩到二维平面上  \n",
    "![词向量二维映射图](word2vec4.jpg)    \n",
    "   * 词向量是用来描述词与词之间的相似程度而构建的高维向量，词向量不仅是一个独立的高维向量，其向量之间通过数学关系包括词意上的联系。而视频，音频数据构建本身意义，可以直接通过数据进行描述图像与声音的相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  练习：  \n",
    "* 加载fasttext中文词向量表：  \n",
    "  1、任意选择词汇，找出与其相似的词  \n",
    "  2、用词汇加减法，找出相对应的词汇   \n",
    "  \n",
    "  -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.RNN/LSTM\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.示例\n",
    "\n",
    "### 利用lstm进行生成莫言小说\n",
    " ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.小结\n",
    "* 自然语言处理常用流程\n",
    "    * 选择处理文本\n",
    "    * 中文分词（了解常用的分词工具及使用方法）\n",
    "    * 选择/训练生成词向量（了解词向量的含义，了解词向量生成过程，会使用预训练的词向量表）\n",
    "    * 训练NLP模型（了解常见的NLP算法）\n",
    "    * 应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
